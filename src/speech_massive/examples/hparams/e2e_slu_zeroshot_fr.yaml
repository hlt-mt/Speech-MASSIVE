adafactor: false
adam_beta1: 0.8
adam_beta2: 0.999
adam_epsilon: 1.0e-09
add_separator: '|'
add_slu_tag: true
data_seed: 32
dataset_name: FBK-MT/Speech-MASSIVE
dataset_config_name: fr-FR
do_eval: false
do_predict: false
do_train: true
eval_dataset_name: FBK-MT/Speech-MASSIVE
eval_dataset_config_name: all
eval_split_name: validation
eval_steps: 100
evaluation_strategy: steps
fp16: false
is_few_shot: false
gradient_accumulation_steps: 1
greater_is_better: false
group_by_length: false
label_smoothing_factor: 0.0
learning_rate: 1.0e-05
length_column_name: length
load_best_model_at_end: true
logging_dir: /your/desired/path/of/log  # change the directory accordingly
logging_steps: 100
logging_strategy: steps
lr_scheduler_type: linear
max_grad_norm: 1.0
max_steps: 20000
metric_for_best_model: eval_loss
model_name_or_path: openai/whisper-medium
overwrite_output_dir: true # should be set False if you wish to continue training from last checkpoint
output_dir: /your/desired/path/of/model # change the directory accordingly
per_device_eval_batch_size: 8
per_device_train_batch_size: 8
predict_with_generate: true
remove_unused_columns: true
save_steps: 100
save_strategy: steps
save_total_limit: 10
seed: 32
target_format_content: transcript_slots_intent
task: transcribe
train_split_name: train
warmup_ratio: 0.0
warmup_steps: 2000
weight_decay: 0.0
